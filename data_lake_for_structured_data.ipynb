{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTRO**\n",
    "\n",
    "As there are many types of data we will go first with the case of  structured data. \n",
    "Structured data is data that follows a schema (column names, data types, relation between entities). Presented most of the time into columnar format such as tables. It results often from transactional databases(OLTP context). But when building a data lake the data modeling also considers analytical databases (OLAP context).\n",
    "\n",
    "Since the core component of our data lake in AWS is the S3 bucket, we will start by creating one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**S3 BUCKET CREATION**\n",
    "\n",
    "To create our S3 bucket we will choose between: \n",
    "\n",
    "- AWS Command Line Interface\n",
    "\n",
    "- AWS Console\n",
    "\n",
    "- AWS API (via boto3)\n",
    "\n",
    "We will choose the AWS CLI because we want to create this bucket from our local machine through a IaC (Infrastructure as Code) tool called *Terrafrom* which is :\n",
    "\n",
    "- Open-source with a large community for support\n",
    "\n",
    "- Very mature and compatible with major cloud providers (AWS,Google and Azure)\n",
    "\n",
    "- It helps us automate the creation process for example if we have to create the same resource many times\n",
    "\n",
    "- It takes advantage of the declarative programming which makes it easy to read for humans\n",
    "\n",
    "To create our S3 bucket we will need the following: an AWS account, a folder where we will create 2 files: *main.tf* and *variables.tf*, an internet connection, the *AWS CLI* installed in our local machine.\n",
    "\n",
    "*The terminal must be opened in the same folder where we have the main.tf and variables.tf files created*\n",
    "\n",
    "**AWS CLI configuration**:\n",
    "\n",
    "We will have to configure our AWS CLI instance to be able to connect to our AWS account from our local machine using the command :\n",
    "\n",
    "aws configure\n",
    "\n",
    "We will be prompted to write our AWS Access Key ID and AWS Secret Access Key (both can be created using the IAM section of our AWS account). Our default region and our default output format will also be required. For our example the default region is us-east-1 and our default output is json.\n",
    "\n",
    "**Terraform main.tf and variables.tf files**:\n",
    "\n",
    "The main.tf and the variables.tf files are files required by terraform to create our AWS resources (or infrastructure). For context, terraform uses a creation plan in 3 phases:\n",
    "\n",
    "- creation plan initialization via the command *'terraform init'* (in the same folder where the main.tf and the variables.tf files are located)\n",
    "\n",
    "- creation plan planification via the command *'terraform plan'*\n",
    "\n",
    "- Execution of the plan via the command *'terraform apply'*\n",
    "\n",
    "The *main.tf* file contains the resource(s) that is/are created with their configurations and specifications, while the *variables.tf* file contains all the variables associated with those resources. It is helpful for the cases where automation is required.\n",
    "\n",
    "Below we will find the content of our main.tf and variables.tf files. Both files are found in the repository of this project on github"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "main.tf:\n",
    "\n",
    "\n",
    "\n",
    "terraform {\n",
    "  required_providers {\n",
    "    aws = {\n",
    "      source = \"hashicorp/aws\"\n",
    "      version = \"~> 5.0\" # Or specify your desired version\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "provider \"aws\" {\n",
    "  region = \"us-east-1\" # Replace with your desired AWS region\n",
    "}\n",
    "\n",
    "resource \"aws_s3_bucket\" \"my_bucket\" {\n",
    "  bucket = var.bucket_name # Use the variable for the bucket name\n",
    "\n",
    "  # Optional: Add other configurations as needed\n",
    "  # acl = \"private\"\n",
    "  # force_destroy = true # Be careful with this!\n",
    "  # versioning {\n",
    "  # enabled = true\n",
    "  # }\n",
    "\n",
    "  tags = {\n",
    "    Name = \"My Terraform Bucket\"\n",
    "    Environment = \"dev\"\n",
    "  }\n",
    "}\n",
    "\n",
    "*# Optional: Block Public Access to Bucket*\n",
    "resource \"aws_s3_bucket_public_access_block\" \"example\" {\n",
    "  bucket = aws_s3_bucket.my_bucket.id\n",
    "  block_public_acls = true\n",
    "  block_public_policy = true\n",
    "  ignore_public_acls = true\n",
    "  restrict_public_buckets = true\n",
    "}\n",
    "\n",
    "output \"bucket_name\" {\n",
    "  value = aws_s3_bucket.my_bucket.id\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "variables.tf:\n",
    "\n",
    "variable \"bucket_name\" {\n",
    "  type = string\n",
    "  description = \"The name of the S3 bucket\"\n",
    "  default = \"mnef-s3-datalake\" # Default value\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the following command are executed without error and in the given order: \n",
    "\n",
    "- terraform init\n",
    "\n",
    "- terraform plan\n",
    "\n",
    "- terraform apply\n",
    "\n",
    "There will be a S3 bucket effectively created within our AWS account.\n",
    "\n",
    "**N.B:** In my account the bucket is named mnef-s3-datalake. Since the bucket name ought to be unique across all AWS, it is recommended to change that name using the variables.tf file, each time the terraform commands are ran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATION OF THE DATA LAKE ZONES**\n",
    "\n",
    "Our S3 bucket being created, we will also create the landing zone, cleaning zone and curated zone. These zones are indeed prefixes(folders) within our S3 bucket. \n",
    "Those folders are created very simply using the S3 service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**NEXT STEP**\n",
    "\n",
    "For the next step we will create the tables that will constitute our structured data to populate our data lake. \n",
    "\n",
    "We will use two well known engines: MySQL and PostgreSQL.\n",
    "The PostgreSQL table will be created on premise, while the MySQL table will be created using AWS  Relational Database Service (RDS). We will also take advantage of the occasion to migrate the PostgreSQL database containing our table within AWS the cloud using Database Migration Service (DMS)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
