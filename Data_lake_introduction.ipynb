{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AWS, MY FIRST DATA LAKE**\n",
    "\n",
    "A *data lake* is a centralized repository for all the data within an organization no matter wht kind it is. Data is stored as-is without needing to be transformed to fit  certain structure. They are very *scalable*(up to petabytes) and support various analytics needs from traditional reporting to advance machine learning. They are subject to data compliance, security and quality requirements.\n",
    "\n",
    "Generally there are cloud providers that are specialized in data lake hosting. Among them AWS is a popular choice due to its wide range of services specifically designed for data lakes, its mature and reliable infrastructure, its scalable storage and the variety of tools for ingesting data from any source (on-premise,streaming data etc..). As far as cost is concerned, AWS offers cost-effectiveness (Pay-as-you-go pricing, storage cost optimization..).Data security and governance are a must in the aws responsible architecture design and it benefits greatly to the data lake creation with features like IAM less-privilege granting policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATALAKE ARCHITECTURE**\n",
    "\n",
    "The main component of our data lake is also its storage component the AWS S3 bucket. \n",
    "It is a file object storage service that is highly scalablea and cost effective.\n",
    "Within our S3 storage we will create 3 prefixes (folders within our S3 bucket), namely a *landing_zone* where ingested data from different sources is stored, a *cleaning_zone* where we will store data from the *landing zone* that has been cleaned, transformed (PII hiding, null handling, duplicates handling, etc...). A *curated_zone* where data from the clean zone that has been enriched is stored for consumption by final stakeholders that are commonly BI teams, data analysts, data scientists.\n",
    "\n",
    "Below a figure representing our data lake architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components of our data lake are :\n",
    "\n",
    "\n",
    "- S3 bucket for highly scalable storage\n",
    "\n",
    "- Data ingestion services that depends of the type of data ingested. A non exhaustive list could be: AWS Glue (structured data), AWS Kinesis data stream (for semi-structured data), AWS Kinesis Video stream (for videos), AWS Managed service for Apache Kafka ( for real time streaming data) etc...\n",
    "\n",
    "- Data Catalog: A map and index for our data lake, making it easier to users to find the data they need, to enforce data governance, data lineage and data quality requirements. It also contains business and technical meta data\n",
    "\n",
    "- Data transformations services that can be used for landing zone to cleaning zone transit as well as for cleaning zone to curated zone transit. We can cite AWS Lambda functions, AWS Glue Studio, AWS Glue Data Brew, AWS Managed service for Apache Flink (for real-time streaming data)... \n",
    "\n",
    "- The consumption layer is formed by BI tools (Tableau, PowerBI), data science and machine learning services (Azure ML, Databricks Spark MLib, AWS SageMaker etc..)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**WORK PLAN** :\n",
    "\n",
    "Our work plan is as follows:\n",
    "\n",
    "1- Creating AWS S3 bucket using Terraform from our local machine\n",
    "\n",
    "2- Using  AWS console to create our different zones (prefixes) within our data lake\n",
    "\n",
    "3- Ingesting data from a MySQL and a Postgresql databases on-premise and from AWS RDS DB instances\n",
    "\n",
    "4- Populate the Data Catalog associated to our data lake using AWS Glue crawlers\n",
    "\n",
    "5- Use AWS databrew and other data transformation services to enrich our data and make it available for the end users \n",
    "\n",
    "6- Query our data using AWS Athena and AWS Redshif Spectrum\n",
    "\n",
    "7- Create and automate a data pipeline for the previous use case with a focus on error handling, monitoring and notification using AWS SNS. We will use either AWS Step functions or AWS manage Worflows for Apache Airflow for pipeline creation\n",
    "\n",
    "8- We will simulate ingestion of steaming data from an IoT device using AWS Kinesis data stream and AWS MSK (Managed Service for Kafka)\n",
    "\n",
    "9- We will store the streamed data within our data lake using AWS Kinesis Firehose\n",
    "\n",
    "10- We will use AWS managed service for Apache Flink or Lambda Apache pyspark to analyse a real time data stream and enrich it with ML/AI models to add value for the final user "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
